{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tbV1No038w0xJfNjEoodAMRm4uZdXZEj",
      "authorship_tag": "ABX9TyP1BTCxNJFDonMn/uqAk6Kt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gazuty/betfair-dashboard/blob/colab-stable-2025-08-10/betfair_dashboard_STABLE_2025_08_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Betfair Dashboard\n",
        "# Built by Gazuty (c) 2025\n",
        "# This notebook processes Betfair profit/loss data, builds analytics tables, and publishes outputs to Google Sheets.\n",
        "\n",
        "# --- STEP 0: Configuration ---\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Your Drive / folder paths ‚îÄ‚îÄ‚îÄ\n",
        "BASE_FOLDER       = '/content/drive/My Drive/Betfair'\n",
        "MASTER_CSV        = os.path.join(BASE_FOLDER, 'Betfair_Master.csv')\n",
        "ARCHIVE_FOLDER    = os.path.join(BASE_FOLDER, 'Archive')\n",
        "BETTING_PATTERN   = os.path.join(BASE_FOLDER, 'BettingPandL*.csv')\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Google Sheet settings ‚îÄ‚îÄ‚îÄ\n",
        "GOOGLE_SHEET_NAME = 'Betfair Dashboard'\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Business rules ‚îÄ‚îÄ‚îÄ\n",
        "VALID_SPORTS      = ['Horse Racing', 'Greyhound Racing']\n",
        "MIN_STRIKE_BETS   = 50\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Setup ‚îÄ‚îÄ‚îÄ\n",
        "os.makedirs(ARCHIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "print(\"\\u2705 Configuration loaded:\")\n",
        "print(f\"  BASE_FOLDER        = {BASE_FOLDER}\")\n",
        "print(f\"  MASTER_CSV         = {MASTER_CSV}\")\n",
        "print(f\"  ARCHIVE_FOLDER     = {ARCHIVE_FOLDER}\")\n",
        "print(f\"  BETTING_PATTERN    = {BETTING_PATTERN}\")\n",
        "print(f\"  GOOGLE_SHEET_NAME  = {GOOGLE_SHEET_NAME}\")\n",
        "print(f\"  VALID_SPORTS       = {VALID_SPORTS}\")\n",
        "print(f\"  MIN_STRIKE_BETS    = {MIN_STRIKE_BETS}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy268ho4Fip2",
        "outputId": "3955b10a-74fc-4a4e-8b6f-1bddb8ae8edb"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded:\n",
            "  BASE_FOLDER        = /content/drive/My Drive/Betfair\n",
            "  MASTER_CSV         = /content/drive/My Drive/Betfair/Betfair_Master.csv\n",
            "  ARCHIVE_FOLDER     = /content/drive/My Drive/Betfair/Archive\n",
            "  BETTING_PATTERN    = /content/drive/My Drive/Betfair/BettingPandL*.csv\n",
            "  GOOGLE_SHEET_NAME  = Betfair Dashboard\n",
            "  VALID_SPORTS       = ['Horse Racing', 'Greyhound Racing']\n",
            "  MIN_STRIKE_BETS    = 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: Master Updater ---\n",
        "\n",
        "import pandas as pd, glob, shutil\n",
        "\n",
        "REQUIRED_COLS = ['Market', 'Settled date']\n",
        "\n",
        "def update_betfair_master():\n",
        "    print(\"\\U0001F504 Starting master update\")\n",
        "\n",
        "    # 1‚É£ Load or initialize master\n",
        "    if os.path.exists(MASTER_CSV):\n",
        "        df_master = pd.read_csv(MASTER_CSV)\n",
        "        df_master['Settled date'] = pd.to_datetime(df_master['Settled date'], errors='coerce')\n",
        "        df_master['Profit_Loss'] = pd.to_numeric(df_master['Profit_Loss'], errors='coerce')\n",
        "        df_master = df_master.dropna(subset=['Settled date']).reset_index(drop=True)\n",
        "        print(f\"\\u2705 Loaded master ({len(df_master)} rows)\")\n",
        "    else:\n",
        "        print(\"\\u26a0 No existing master found ‚Äî starting fresh\")\n",
        "        df_master = pd.DataFrame(columns=REQUIRED_COLS + ['Profit_Loss'])\n",
        "\n",
        "    # 2‚É£ Gather raw files\n",
        "    raw_files = glob.glob(BETTING_PATTERN)\n",
        "    print(f\"üìÇ Found {len(raw_files)} raw file(s)\")\n",
        "\n",
        "    if not raw_files:\n",
        "        print(\"\\u26a0 No raw files to process ‚Äî exiting.\")\n",
        "        return\n",
        "\n",
        "    # 3‚É£ Process each file\n",
        "    dfs = []\n",
        "    for filepath in raw_files:\n",
        "        fname = os.path.basename(filepath)\n",
        "        print(f\"üìÖ {fname}\", end=\"\")\n",
        "\n",
        "        df = pd.read_csv(filepath)\n",
        "        missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
        "        if missing:\n",
        "            print(f\" ‚Üí ‚ùå missing columns {missing}\")\n",
        "            continue\n",
        "\n",
        "        profs = [c for c in df.columns if 'profit' in c.lower()]\n",
        "        if not profs:\n",
        "            print(\" ‚Üí ‚ùå no profit column found\")\n",
        "            continue\n",
        "\n",
        "        pick = next((c for c in profs if 'aud' in c.lower()), profs[0])\n",
        "        df['Profit_Loss'] = pd.to_numeric(df[pick], errors='coerce')\n",
        "        df['Settled date'] = pd.to_datetime(df['Settled date'], errors='coerce')\n",
        "        df = df[['Market', 'Settled date', 'Profit_Loss']].dropna(subset=['Settled date'])\n",
        "\n",
        "        dfs.append(df)\n",
        "        print(f\" ‚Üí {len(df)} rows from '{pick}'\")\n",
        "\n",
        "    if not dfs:\n",
        "        print(\"\\u26a0 No valid data loaded from raw files ‚Äî exiting.\")\n",
        "        return\n",
        "\n",
        "    # 4‚É£ Combine and deduplicate\n",
        "    df_new = pd.concat(dfs, ignore_index=True)\n",
        "    df_new = df_new.dropna(subset=['Settled date']).reset_index(drop=True)\n",
        "\n",
        "    df_master['_key'] = (\n",
        "        df_master['Market'].astype(str) + \"|\" +\n",
        "        df_master['Settled date'].dt.strftime('%Y-%m-%d %H:%M:%S') + \"|\" +\n",
        "        df_master['Profit_Loss'].astype(str)\n",
        "    )\n",
        "    df_new['_key'] = (\n",
        "        df_new['Market'].astype(str) + \"|\" +\n",
        "        df_new['Settled date'].dt.strftime('%Y-%m-%d %H:%M:%S') + \"|\" +\n",
        "        df_new['Profit_Loss'].astype(str)\n",
        "    )\n",
        "\n",
        "    df_unique = df_new[~df_new['_key'].isin(df_master['_key'])]\n",
        "    print(f\"\\u2705 {len(df_unique)} unique new row(s) identified\")\n",
        "\n",
        "    # 5‚É£ Merge and save\n",
        "    if not df_unique.empty:\n",
        "        df_combined = pd.concat([\n",
        "            df_master.drop(columns=['_key']),\n",
        "            df_unique.drop(columns=['_key'])\n",
        "        ], ignore_index=True)\n",
        "        df_combined.to_csv(MASTER_CSV, index=False)\n",
        "        print(f\"\\u2705 Master updated ({len(df_combined)} rows) ‚Üí {MASTER_CSV}\")\n",
        "    else:\n",
        "        print(\"\\u26a0 No new rows to add ‚Äî master unchanged.\")\n",
        "\n",
        "    # 6‚É£ Archive files\n",
        "    for filepath in raw_files:\n",
        "        fname = os.path.basename(filepath)\n",
        "        shutil.move(filepath, os.path.join(ARCHIVE_FOLDER, fname))\n",
        "        print(f\"üì¶ Archived {fname}\")\n",
        "\n",
        "# Run the function\n",
        "update_betfair_master()\n"
      ],
      "metadata": {
        "id": "6Bgp_pFfIjKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2462b7-9dba-489d-aa5d-e78ae569a830"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Starting master update\n",
            "‚úÖ Loaded master (22387 rows)\n",
            "üìÇ Found 0 raw file(s)\n",
            "‚ö† No raw files to process ‚Äî exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jz5OxZ8aw3Z3"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: Load Master ---\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"üìÇ Loading master from: {MASTER_CSV}\")\n",
        "df = pd.read_csv(MASTER_CSV)\n",
        "\n",
        "# Ensure correct dtypes\n",
        "df['Settled date'] = pd.to_datetime(df['Settled date'], errors='coerce')\n",
        "df['Profit_Loss'] = pd.to_numeric(df['Profit_Loss'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid dates\n",
        "before = len(df)\n",
        "df = df.dropna(subset=['Settled date']).reset_index(drop=True)\n",
        "after = len(df)\n",
        "\n",
        "print(f\"‚úÖ {after} rows loaded (dropped {before - after} invalid dates).\")\n",
        "print(f\"   Profit_Loss dtype: {df['Profit_Loss'].dtype}\")\n"
      ],
      "metadata": {
        "id": "Ag8x2FTv98Bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921095bb-9a47-4708-89cb-66a0285fce85"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading master from: /content/drive/My Drive/Betfair/Betfair_Master.csv\n",
            "‚úÖ 22387 rows loaded (dropped 0 invalid dates).\n",
            "   Profit_Loss dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: Feature Extraction ---\n",
        "\n",
        "# 1Ô∏è‚É£ Extract Sport from Market (first token before slash)\n",
        "df['Sport'] = df['Market'].str.extract(r'^([^/]+)/')[0].str.strip()\n",
        "\n",
        "# 2Ô∏è‚É£ Extract Track_Info and Event_Description for racing sports\n",
        "racing_mask = df['Sport'].isin(VALID_SPORTS)\n",
        "track_event = df.loc[racing_mask, 'Market'].str.extract(r'/\\s*(.*?)\\s*:\\s*(.*)')\n",
        "track_event.columns = ['Track_Info', 'Event_Description']\n",
        "df.loc[racing_mask, ['Track_Info', 'Event_Description']] = track_event\n",
        "\n",
        "# 3Ô∏è‚É£ Extract Country from parentheses in Track_Info\n",
        "df['Country'] = df['Track_Info'].str.extract(r'\\(([^)]+)\\)')[0]\n",
        "\n",
        "# 4Ô∏è‚É£ Fill missing country values\n",
        "df['Country'] = df['Country'].fillna('UK')\n",
        "df.loc[~df['Sport'].isin(VALID_SPORTS), 'Country'] = 'Unknown'\n",
        "\n",
        "# 5Ô∏è‚É£ Clean up Track_Info to produce Track_Name (remove dates and country)\n",
        "df['Track_Name'] = (\n",
        "    df['Track_Info']\n",
        "      .str.replace(r'\\([^)]*\\)', '', regex=True)\n",
        "      .str.replace(r'\\b\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+\\b', '', regex=True)\n",
        "      .str.strip()\n",
        ")\n",
        "\n",
        "# 6Ô∏è‚É£ Preview output\n",
        "preview = df.loc[df['Track_Name'].notna(), ['Sport', 'Track_Name', 'Country']].drop_duplicates().head(10)\n",
        "print(\"‚úÖ Feature extraction complete ‚Äî first few tracks:\")\n",
        "print(preview)\n"
      ],
      "metadata": {
        "id": "3h7weeM--HWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8a3e4e-bc22-4508-8948-b605c9f5e0a8"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Feature extraction complete ‚Äî first few tracks:\n",
            "               Sport     Track_Name Country\n",
            "0       Horse Racing      Uttoxeter      UK\n",
            "1   Greyhound Racing       Mandurah     AUS\n",
            "2       Horse Racing      Lingfield      UK\n",
            "3       Horse Racing        Warwick      UK\n",
            "9   Greyhound Racing        Monmore      UK\n",
            "11  Greyhound Racing        Romford      UK\n",
            "20      Horse Racing   Philadelphia      US\n",
            "23      Horse Racing  Turf Paradise      US\n",
            "24      Horse Racing       Riverton     NZL\n",
            "25      Horse Racing      Ellerslie     NZL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: Build summary tables (daily, weekly, monthly, sport, country) ---\n",
        "\n",
        "print(\"üîß STEP 4: Building summary tables...\")\n",
        "\n",
        "# 4.1Ô∏è‚É£ Daily Summary (chronological)\n",
        "by_day = (\n",
        "    df.groupby(df['Settled date'].dt.date)['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index(name='Profit_Loss')\n",
        "      .rename(columns={'Settled date': 'Day'})\n",
        ")\n",
        "by_day = by_day.sort_values('Day').reset_index(drop=True)\n",
        "by_day['Cumulative_Profit_Loss'] = by_day['Profit_Loss'].cumsum()\n",
        "by_day[['Profit_Loss', 'Cumulative_Profit_Loss']] = by_day[['Profit_Loss', 'Cumulative_Profit_Loss']].round(2)\n",
        "\n",
        "# 4.2Ô∏è‚É£ Rolling Returns (start from 1 March)\n",
        "by_day['Day'] = pd.to_datetime(by_day['Day'])\n",
        "rolling_start = pd.to_datetime('2025-03-01')\n",
        "rolling_df = by_day[by_day['Day'] >= rolling_start].copy()\n",
        "rolling_df = rolling_df.set_index('Day')\n",
        "\n",
        "rolling_df['Rolling 2w'] = by_day.set_index('Day')['Profit_Loss'].rolling(window='14D').sum()\n",
        "rolling_df['Rolling 4w'] = by_day.set_index('Day')['Profit_Loss'].rolling(window='28D').sum()\n",
        "rolling_df['Rolling 8w'] = by_day.set_index('Day')['Profit_Loss'].rolling(window='56D').sum()\n",
        "\n",
        "rolling_df = rolling_df.drop(columns=['Profit_Loss', 'Cumulative_Profit_Loss']).reset_index()\n",
        "rolling_df = rolling_df.round(2)\n",
        "rolling_df.columns = ['Day', 'Rolling 2w', 'Rolling 4w', 'Rolling 8w']\n",
        "\n",
        "# 4.3Ô∏è‚É£ Weekly Summary (week starts Sunday)\n",
        "by_week = (\n",
        "    df.set_index('Settled date')\n",
        "      .resample('W-SUN')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .rename(columns={'Settled date': 'Week Starting'})\n",
        ")\n",
        "by_week['Profit_Loss'] = by_week['Profit_Loss'].round(2)\n",
        "\n",
        "# 4.4Ô∏è‚É£ Rolling by Sport (Horse Racing & Greyhounds)\n",
        "rolling_by_sport = {}\n",
        "for sport in ['Horse Racing', 'Greyhound Racing']:\n",
        "    sport_df = df[df['Sport'] == sport].copy()\n",
        "    sport_by_day = (\n",
        "        sport_df.groupby(sport_df['Settled date'].dt.date)['Profit_Loss']\n",
        "        .sum()\n",
        "        .reset_index(name='Profit_Loss')\n",
        "        .rename(columns={'Settled date': 'Day'})\n",
        "        .sort_values('Day')\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    sport_by_day['Day'] = pd.to_datetime(sport_by_day['Day'])\n",
        "    sport_by_day = sport_by_day.set_index('Day')\n",
        "\n",
        "    result = sport_by_day.copy()\n",
        "    result['Rolling 2w'] = sport_by_day['Profit_Loss'].rolling(window='14D').sum()\n",
        "    result['Rolling 4w'] = sport_by_day['Profit_Loss'].rolling(window='28D').sum()\n",
        "    result['Rolling 8w'] = sport_by_day['Profit_Loss'].rolling(window='56D').sum()\n",
        "\n",
        "    result = result.drop(columns=['Profit_Loss']).reset_index()\n",
        "    result = result[result['Day'] >= rolling_start].round(2)\n",
        "    result.columns = ['Day', 'Rolling 2w', 'Rolling 4w', 'Rolling 8w']\n",
        "    rolling_by_sport[sport] = result\n",
        "\n",
        "# 4.5Ô∏è‚É£ Monthly Summary ‚Äî using 'ME' to avoid deprecation warning\n",
        "by_month = (\n",
        "    df.set_index('Settled date')\n",
        "      .resample('ME')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        ")\n",
        "by_month['Month'] = by_month['Settled date'].dt.to_period('M').astype(str)\n",
        "by_month = by_month[['Month', 'Profit_Loss']]\n",
        "by_month['Profit_Loss'] = by_month['Profit_Loss'].round(2)\n",
        "\n",
        "# 4.6Ô∏è‚É£ Sport Summary\n",
        "by_sport = (\n",
        "    df.groupby('Sport')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .round({'Profit_Loss': 2})\n",
        ")\n",
        "\n",
        "# 4.7Ô∏è‚É£ Country Summary\n",
        "by_country = (\n",
        "    df.groupby('Country')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .round({'Profit_Loss': 2})\n",
        ")\n",
        "\n",
        "# 4.8Ô∏è‚É£ Daily Summaries per Sport (with cumulative P/L)\n",
        "sport_daily = {}\n",
        "for sport in df['Sport'].dropna().unique():\n",
        "    temp = (\n",
        "        df[df['Sport'] == sport]\n",
        "          .groupby(df['Settled date'].dt.date)['Profit_Loss']\n",
        "          .sum()\n",
        "          .reset_index(name='Profit_Loss')\n",
        "          .rename(columns={'Settled date': 'Day'})\n",
        "          .sort_values('Day')\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    temp['Cumulative_Profit_Loss'] = temp['Profit_Loss'].cumsum().round(2)\n",
        "    temp['Profit_Loss'] = temp['Profit_Loss'].round(2)\n",
        "    sport_daily[f\"{sport} Daily\"] = temp\n",
        "\n",
        "# ‚úÖ Summary Checks\n",
        "print(f\"‚úÖ By Day: {len(by_day)} rows (last: {by_day['Day'].max().date()})\")\n",
        "print(f\"‚úÖ Rolling Returns: {len(rolling_df)} rows (last: {rolling_df['Day'].max().date()})\")\n",
        "for k, v in rolling_by_sport.items():\n",
        "    print(f\"‚úÖ Rolling {k}: {len(v)} rows (last: {v['Day'].max().date()})\")\n",
        "print(f\"‚úÖ By Week: {len(by_week)} rows (last: {by_week['Week Starting'].max().date()})\")\n",
        "print(f\"‚úÖ By Month: {len(by_month)} rows (last: {by_month['Month'].max()})\")\n",
        "print(f\"‚úÖ By Sport: {len(by_sport)} sports ‚Üí {by_sport['Sport'].tolist()}\")\n",
        "print(f\"‚úÖ By Country: {len(by_country)} countries ‚Üí {by_country['Country'].tolist()}\")\n"
      ],
      "metadata": {
        "id": "V_K4tw9-cWvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67dd137-e627-483a-88c2-a48f0a5dd23f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß STEP 4: Building summary tables...\n",
            "‚úÖ By Day: 222 rows (last: 2025-08-10)\n",
            "‚úÖ Rolling Returns: 163 rows (last: 2025-08-10)\n",
            "‚úÖ Rolling Horse Racing: 163 rows (last: 2025-08-10)\n",
            "‚úÖ Rolling Greyhound Racing: 163 rows (last: 2025-08-10)\n",
            "‚úÖ By Week: 32 rows (last: 2025-08-10)\n",
            "‚úÖ By Month: 8 rows (last: 2025-08)\n",
            "‚úÖ By Sport: 16 sports ‚Üí ['American Football', 'Basketball', 'Cricket', 'Cycling', 'Darts', 'Football', 'Gaelic Games', 'Golf', 'Greyhound Racing', 'Horse Racing', 'Ice Hockey', 'Motor Sport', 'Politics', 'Rugby Union', 'Snooker', 'Tennis']\n",
            "‚úÖ By Country: 8 countries ‚Üí ['AUS', 'FRA', 'NZL', 'RSA', 'UAE', 'UK', 'US', 'Unknown']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: Track Summaries ---\n",
        "\n",
        "# 1Ô∏è‚É£ Aggregate P/L per track for Horse and Greyhound Racing\n",
        "track_df = (\n",
        "    df[df['Sport'].isin(VALID_SPORTS)]\n",
        "      .groupby(['Sport', 'Track_Name'], as_index=False)['Profit_Loss']\n",
        "      .sum()\n",
        ")\n",
        "track_df['Profit_Loss'] = track_df['Profit_Loss'].round(2)\n",
        "\n",
        "# 2Ô∏è‚É£ Create summary groups\n",
        "tracks = {\n",
        "    'Track Stats':               track_df,\n",
        "    'Top Horse Tracks':          track_df.query(\"Sport == 'Horse Racing'\").nlargest(15, 'Profit_Loss'),\n",
        "    'Bottom Horse Tracks':       track_df.query(\"Sport == 'Horse Racing'\").nsmallest(15, 'Profit_Loss'),\n",
        "    'Top Greyhound Tracks':      track_df.query(\"Sport == 'Greyhound Racing'\").nlargest(15, 'Profit_Loss'),\n",
        "    'Bottom Greyhound Tracks':   track_df.query(\"Sport == 'Greyhound Racing'\").nsmallest(15, 'Profit_Loss'),\n",
        "}\n",
        "\n",
        "# 3Ô∏è‚É£ Preview sample\n",
        "print(\"‚úÖ Track summaries built.\")\n",
        "print(\" ‚Ä¢ Sample Track Stats:\")\n",
        "print(track_df.head())\n",
        "print(\" ‚Ä¢ Top Horse Tracks:\")\n",
        "print(tracks['Top Horse Tracks'][['Track_Name', 'Profit_Loss']].head())\n"
      ],
      "metadata": {
        "id": "nFAP0CNsccL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841e2ae7-1343-4e46-edb9-768f44c4d486"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Track summaries built.\n",
            " ‚Ä¢ Sample Track Stats:\n",
            "              Sport   Track_Name  Profit_Loss\n",
            "0  Greyhound Racing  Albion Park        91.77\n",
            "1  Greyhound Racing   Angle Park      -129.51\n",
            "2  Greyhound Racing     Ballarat        42.44\n",
            "3  Greyhound Racing      Bendigo         3.01\n",
            "4  Greyhound Racing  Broken Hill        58.85\n",
            " ‚Ä¢ Top Horse Tracks:\n",
            "    Track_Name  Profit_Loss\n",
            "174  Geraldton      1400.78\n",
            "308   Rosehill      1367.26\n",
            "57     Aintree      1286.07\n",
            "327  Southwell      1104.96\n",
            "265  Newcastle       974.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: Strike Rates ---\n",
        "\n",
        "# 1Ô∏è‚É£ Filter to Horse & Greyhound Racing\n",
        "df_racing = df[df['Sport'].isin(VALID_SPORTS)].copy()\n",
        "\n",
        "# 2Ô∏è‚É£ Compute total bets and wins per track\n",
        "strike_df = (\n",
        "    df_racing\n",
        "      .groupby(['Sport', 'Track_Name'])['Profit_Loss']\n",
        "      .agg(\n",
        "          total_bets='count',\n",
        "          wins=lambda x: (x > 0).sum()\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Calculate strike rate\n",
        "strike_df['Strike_Rate'] = (strike_df['wins'] / strike_df['total_bets']).round(4)\n",
        "\n",
        "# 4Ô∏è‚É£ Filter by minimum bets threshold\n",
        "strike_df_filtered = strike_df[strike_df['total_bets'] >= MIN_STRIKE_BETS].reset_index(drop=True)\n",
        "\n",
        "# 5Ô∏è‚É£ Extract Top & Bottom Strike Rate Tracks\n",
        "top_strike    = strike_df_filtered.nlargest(10, 'Strike_Rate').reset_index(drop=True)\n",
        "bottom_strike = strike_df_filtered.nsmallest(10, 'Strike_Rate').reset_index(drop=True)\n",
        "\n",
        "# 6Ô∏è‚É£ Preview\n",
        "print(f\"‚úÖ Strike rates computed (min {MIN_STRIKE_BETS} bets):\")\n",
        "print(\"Top 10 Strike Rates:\")\n",
        "print(top_strike[['Sport', 'Track_Name', 'total_bets', 'wins', 'Strike_Rate']])\n",
        "print(\"\\nBottom 10 Strike Rates:\")\n",
        "print(bottom_strike[['Sport', 'Track_Name', 'total_bets', 'wins', 'Strike_Rate']])\n"
      ],
      "metadata": {
        "id": "YrBPZlTccebQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a8c623-0f88-499a-f1b7-55376907749c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Strike rates computed (min 50 bets):\n",
            "Top 10 Strike Rates:\n",
            "          Sport Track_Name  total_bets  wins  Strike_Rate\n",
            "0  Horse Racing   Rosehill          92    75       0.8152\n",
            "1  Horse Racing    Chester          72    57       0.7917\n",
            "2  Horse Racing       York          73    55       0.7534\n",
            "3  Horse Racing   Saratoga         112    83       0.7411\n",
            "4  Horse Racing  Ellerslie          55    40       0.7273\n",
            "5  Horse Racing  Chantilly          74    53       0.7162\n",
            "6  Horse Racing       Bath          55    39       0.7091\n",
            "7  Horse Racing   Brighton          78    55       0.7051\n",
            "8  Horse Racing    Newbury          93    65       0.6989\n",
            "9  Horse Racing   Woodbine          63    44       0.6984\n",
            "\n",
            "Bottom 10 Strike Rates:\n",
            "              Sport    Track_Name  total_bets  wins  Strike_Rate\n",
            "0  Greyhound Racing        Hobart         157    72       0.4586\n",
            "1  Greyhound Racing  Q2 Parklands          95    45       0.4737\n",
            "2      Horse Racing   Turffontein          95    45       0.4737\n",
            "3  Greyhound Racing    Launceston         182    88       0.4835\n",
            "4  Greyhound Racing    Shepparton         155    75       0.4839\n",
            "5      Horse Racing     Greyville          70    34       0.4857\n",
            "6  Greyhound Racing       Bendigo         182    91       0.5000\n",
            "7      Horse Racing  Charles Town          58    29       0.5000\n",
            "8      Horse Racing    Kenilworth          90    45       0.5000\n",
            "9      Horse Racing  Market Rasen          52    26       0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7: Prepare all_sheets for export ---\n",
        "\n",
        "# 7.1Ô∏è‚É£ Ensure all_sheets exists\n",
        "if 'all_sheets' not in locals():\n",
        "    all_sheets = {}\n",
        "\n",
        "# 7.2Ô∏è‚É£ Add core summaries\n",
        "all_sheets.update({\n",
        "    'By Day':           by_day,\n",
        "    'By Day Sorted':    by_day.sort_values('Profit_Loss', ascending=False).reset_index(drop=True),\n",
        "    'By Week':          by_week,\n",
        "    'Cumulative':       by_day[['Day', 'Cumulative_Profit_Loss']].rename(columns={'Cumulative_Profit_Loss': 'Cumulative'}),\n",
        "    'By Month':         by_month,\n",
        "    'By Sport':         by_sport,\n",
        "    'By Country':       by_country,\n",
        "    'Rolling Returns':  rolling_df\n",
        "})\n",
        "\n",
        "# 7.3Ô∏è‚É£ Add track-level summaries\n",
        "all_sheets.update({\n",
        "    'Track Stats':             tracks['Track Stats'],\n",
        "    'Top Horse Tracks':        tracks['Top Horse Tracks'],\n",
        "    'Bottom Horse Tracks':     tracks['Bottom Horse Tracks'],\n",
        "    'Top Greyhound Tracks':    tracks['Top Greyhound Tracks'],\n",
        "    'Bottom Greyhound Tracks': tracks['Bottom Greyhound Tracks'],\n",
        "})\n",
        "\n",
        "# 7.4Ô∏è‚É£ Add strike rate summaries\n",
        "all_sheets.update({\n",
        "    'Top Strike Rates':    top_strike,\n",
        "    'Bottom Strike Rates': bottom_strike,\n",
        "})\n",
        "\n",
        "# 7.5Ô∏è‚É£ Add daily summaries for each sport\n",
        "all_sheets.update(sport_daily)\n",
        "\n",
        "# 7.6Ô∏è‚É£ Add rolling returns by sport\n",
        "for sport, df_rolling in rolling_by_sport.items():\n",
        "    sheet_name = f\"Rolling {sport}\"\n",
        "    all_sheets[sheet_name] = df_rolling\n",
        "\n",
        "# 7.7Ô∏è‚É£ Final review of included sheets\n",
        "print(f\"‚úÖ Prepared {len(all_sheets)} tables for export:\")\n",
        "for name in all_sheets:\n",
        "    print(f\"  ‚Ä¢ {name}\")\n"
      ],
      "metadata": {
        "id": "OP-sN4D9cgqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6579ce46-4a5d-4c41-c794-c16f15dd85fa"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prepared 33 tables for export:\n",
            "  ‚Ä¢ By Day\n",
            "  ‚Ä¢ By Day Sorted\n",
            "  ‚Ä¢ By Week\n",
            "  ‚Ä¢ Cumulative\n",
            "  ‚Ä¢ By Month\n",
            "  ‚Ä¢ By Sport\n",
            "  ‚Ä¢ By Country\n",
            "  ‚Ä¢ Rolling Returns\n",
            "  ‚Ä¢ Track Stats\n",
            "  ‚Ä¢ Top Horse Tracks\n",
            "  ‚Ä¢ Bottom Horse Tracks\n",
            "  ‚Ä¢ Top Greyhound Tracks\n",
            "  ‚Ä¢ Bottom Greyhound Tracks\n",
            "  ‚Ä¢ Top Strike Rates\n",
            "  ‚Ä¢ Bottom Strike Rates\n",
            "  ‚Ä¢ Snooker Daily\n",
            "  ‚Ä¢ Ice Hockey Daily\n",
            "  ‚Ä¢ Horse Racing Daily\n",
            "  ‚Ä¢ Golf Daily\n",
            "  ‚Ä¢ Politics Daily\n",
            "  ‚Ä¢ Tennis Daily\n",
            "  ‚Ä¢ Greyhound Racing Daily\n",
            "  ‚Ä¢ Football Daily\n",
            "  ‚Ä¢ Motor Sport Daily\n",
            "  ‚Ä¢ Cricket Daily\n",
            "  ‚Ä¢ Darts Daily\n",
            "  ‚Ä¢ Basketball Daily\n",
            "  ‚Ä¢ American Football Daily\n",
            "  ‚Ä¢ Rugby Union Daily\n",
            "  ‚Ä¢ Cycling Daily\n",
            "  ‚Ä¢ Gaelic Games Daily\n",
            "  ‚Ä¢ Rolling Horse Racing\n",
            "  ‚Ä¢ Rolling Greyhound Racing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä Top Horse Tracks preview:\")\n",
        "print(tracks['Top Horse Tracks'].head())\n",
        "print(\"üìä Bottom Horse Tracks preview:\")\n",
        "print(tracks['Bottom Horse Tracks'].head())\n"
      ],
      "metadata": {
        "id": "NfU0nlBeNY4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5768646b-b4c8-4c21-da98-51847e98d201"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Top Horse Tracks preview:\n",
            "            Sport Track_Name  Profit_Loss\n",
            "174  Horse Racing  Geraldton      1400.78\n",
            "308  Horse Racing   Rosehill      1367.26\n",
            "57   Horse Racing    Aintree      1286.07\n",
            "327  Horse Racing  Southwell      1104.96\n",
            "265  Horse Racing  Newcastle       974.57\n",
            "üìä Bottom Horse Tracks preview:\n",
            "            Sport    Track_Name  Profit_Loss\n",
            "356  Horse Racing  Turfway Park      -336.93\n",
            "303  Horse Racing         Ripon      -176.58\n",
            "374  Horse Racing     Wincanton      -129.13\n",
            "372  Horse Racing      Wetherby      -103.88\n",
            "116  Horse Racing  Charles Town       -82.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick safety check: do master (df) totals for yesterday/today agree with any raw files present?\n",
        "import pandas as pd, glob, pytz\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "assert 'df' in globals(), \"Please run Steps 2‚Äì7 first (df should be in memory).\"\n",
        "\n",
        "BASE_FOLDER     = '/content/drive/My Drive/Betfair'\n",
        "BETTING_PATTERN = f'{BASE_FOLDER}/BettingPandL*.csv'\n",
        "\n",
        "au = pytz.timezone(\"Australia/Sydney\")\n",
        "y  = (datetime.now(au) - timedelta(days=1)).date()\n",
        "t  = datetime.now(au).date()\n",
        "\n",
        "# Master totals\n",
        "df_m = df.copy()\n",
        "if not pd.api.types.is_datetime64_any_dtype(df_m['Settled date']):\n",
        "    df_m['Settled date'] = pd.to_datetime(df_m['Settled date'], errors='coerce')\n",
        "by_day_master = df_m.groupby(df_m['Settled date'].dt.date)['Profit_Loss'].sum()\n",
        "m_y = float(by_day_master.get(y, 0.0))\n",
        "m_t = float(by_day_master.get(t, 0.0))\n",
        "\n",
        "# Raw totals (if any files are still in the folder)\n",
        "raw_files = sorted(glob.glob(BETTING_PATTERN))\n",
        "r_y = r_t = 0.0\n",
        "if raw_files:\n",
        "    raws = []\n",
        "    for p in raw_files:\n",
        "        r = pd.read_csv(p)\n",
        "        if 'Settled date' in r and any(c in r.columns for c in ['Profit/Loss (AUD)', 'Profit_Loss']):\n",
        "            plcol = 'Profit/Loss (AUD)' if 'Profit/Loss (AUD)' in r.columns else 'Profit_Loss'\n",
        "            r = r.rename(columns={plcol: 'Profit_Loss'})\n",
        "            r['Settled date'] = pd.to_datetime(r['Settled date'], errors='coerce')\n",
        "            r['Profit_Loss']  = pd.to_numeric(r['Profit_Loss'], errors='coerce')\n",
        "            raws.append(r[['Settled date','Profit_Loss']])\n",
        "    if raws:\n",
        "        R = pd.concat(raws, ignore_index=True).dropna(subset=['Settled date'])\n",
        "        R['day'] = R['Settled date'].dt.date\n",
        "        r_y = float(R.loc[R['day']==y, 'Profit_Loss'].sum())\n",
        "        r_t = float(R.loc[R['day']==t, 'Profit_Loss'].sum())\n",
        "\n",
        "print(f\"MASTER ‚Üí {y}: {m_y:.2f} | {t}: {m_t:.2f}\")\n",
        "print(f\" RAWS  ‚Üí {y}: {r_y:.2f} | {t}: {r_t:.2f}  (0.00 if no raw files)\")\n",
        "\n",
        "ok = (abs(m_y - r_y) < 0.01 or r_y == 0.0) and (abs(m_t - r_t) < 0.01 or r_t == 0.0)\n",
        "print(\"PASS ‚úÖ ‚Äî proceed to Step 8\" if ok else \"STOP ‚ùå ‚Äî mismatch; tell me these numbers\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3h3bS7V05lY",
        "outputId": "11dfdf27-1c1f-4df5-8adb-ec0f72d45954"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASTER ‚Üí 2025-08-09: 529.21 | 2025-08-10: 172.41\n",
            " RAWS  ‚Üí 2025-08-09: 0.00 | 2025-08-10: 0.00  (0.00 if no raw files)\n",
            "PASS ‚úÖ ‚Äî proceed to Step 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 8 (safe/minimal writes): Export to Google Sheets ---\n",
        "\n",
        "import time, gspread, pandas as pd\n",
        "from datetime import date\n",
        "from gspread.exceptions import APIError\n",
        "\n",
        "SERVICE_JSON = \"/content/drive/My Drive/Betfair/testsheets-257205-11522dd72797.json\"\n",
        "SHEET_ID     = \"1Ia9OUbft4KtMgZd3kuMGHs267uhzTmpmzJqR5CkSXTM\"  # or open by title\n",
        "\n",
        "# ---- Helpers ----\n",
        "def values_from_df(df: pd.DataFrame):\n",
        "    out = df.copy()\n",
        "    # round numeric, stringify everything for a single update call\n",
        "    for c in out.select_dtypes(include=['float', 'int']).columns:\n",
        "        out[c] = pd.to_numeric(out[c], errors='coerce').round(2)\n",
        "    return [out.columns.tolist()] + out.fillna(\"\").astype(str).values.tolist()\n",
        "\n",
        "def retry_gs(call, *args, **kwargs):\n",
        "    delay = 3\n",
        "    for _ in range(6):  # ~ up to ~1 min\n",
        "        try:\n",
        "            return call(*args, **kwargs)\n",
        "        except APIError as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(f\"‚è≥ Hit Sheets quota; retrying in {delay}s‚Ä¶\")\n",
        "                time.sleep(delay)\n",
        "                delay = min(60, delay * 2)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# ---- Connect ----\n",
        "gc = gspread.service_account(filename=SERVICE_JSON)\n",
        "sh = gc.open_by_key(SHEET_ID)  # or: gc.open(GOOGLE_SHEET_NAME)\n",
        "print(f\"‚úÖ Connected to '{sh.title}'\")\n",
        "\n",
        "# ---- Upload each prepared table with ONE update, no resize/clear ----\n",
        "PER_SHEET_PAUSE = 1.2  # gentle pacing\n",
        "for name, df_out in all_sheets.items():\n",
        "    try:\n",
        "        ws = sh.worksheet(name)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        # create once with a reasonable size; avoid resizing later\n",
        "        rows = max(len(df_out) + 5, 1000)\n",
        "        cols = max(len(df_out.columns) + 5, 20)\n",
        "        ws = retry_gs(sh.add_worksheet, title=name, rows=rows, cols=cols)\n",
        "\n",
        "    vals = values_from_df(df_out)\n",
        "    retry_gs(ws.update, vals, range_name=\"A1\", value_input_option=\"RAW\")\n",
        "    print(f\"‚úÖ Uploaded tab: {name}\")\n",
        "    time.sleep(PER_SHEET_PAUSE)\n",
        "\n",
        "# ---- Dashboard KPIs (single write; no resize) ----\n",
        "df_kpi = df.copy()\n",
        "if 'Settled date' in df_kpi and not pd.api.types.is_datetime64_any_dtype(df_kpi['Settled date']):\n",
        "    df_kpi['Settled date'] = pd.to_datetime(df_kpi['Settled date'], errors='coerce')\n",
        "if 'Profit_Loss' in df_kpi and not pd.api.types.is_numeric_dtype(df_kpi['Profit_Loss']):\n",
        "    df_kpi['Profit_Loss'] = pd.to_numeric(df_kpi['Profit_Loss'], errors='coerce')\n",
        "\n",
        "total_profit = round(float(df_kpi['Profit_Loss'].sum()), 2)\n",
        "total_bets   = int(len(df_kpi))\n",
        "by_day = df_kpi.groupby(df_kpi['Settled date'].dt.date)['Profit_Loss'].sum()\n",
        "best_day  = str(by_day.idxmax()) if not by_day.empty else \"\"\n",
        "worst_day = str(by_day.idxmin()) if not by_day.empty else \"\"\n",
        "\n",
        "kpis = [\n",
        "    ['Metric', 'Value'],\n",
        "    ['Total Profit/Loss', total_profit],\n",
        "    ['Number of Bets', total_bets],\n",
        "    ['Best Day', best_day],\n",
        "    ['Worst Day', worst_day],\n",
        "    ['Generated on', str(date.today())],\n",
        "]\n",
        "\n",
        "try:\n",
        "    dash = sh.worksheet('Dashboard')\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    dash = retry_gs(sh.add_worksheet, title='Dashboard', rows=20, cols=5)\n",
        "\n",
        "retry_gs(dash.update, kpis, range_name=\"A1\", value_input_option=\"RAW\")\n",
        "print(\"‚úÖ Dashboard KPIs updated (no resize)\")\n"
      ],
      "metadata": {
        "id": "sw42u_CuCOsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075ade84-fa4c-491a-cf70-ba855bbba512"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Connected to 'Betfair Dashboard'\n",
            "‚úÖ Uploaded tab: By Day\n",
            "‚úÖ Uploaded tab: By Day Sorted\n",
            "‚úÖ Uploaded tab: By Week\n",
            "‚úÖ Uploaded tab: Cumulative\n",
            "‚úÖ Uploaded tab: By Month\n",
            "‚úÖ Uploaded tab: By Sport\n",
            "‚úÖ Uploaded tab: By Country\n",
            "‚úÖ Uploaded tab: Rolling Returns\n",
            "‚úÖ Uploaded tab: Track Stats\n",
            "‚úÖ Uploaded tab: Top Horse Tracks\n",
            "‚úÖ Uploaded tab: Bottom Horse Tracks\n",
            "‚úÖ Uploaded tab: Top Greyhound Tracks\n",
            "‚úÖ Uploaded tab: Bottom Greyhound Tracks\n",
            "‚úÖ Uploaded tab: Top Strike Rates\n",
            "‚úÖ Uploaded tab: Bottom Strike Rates\n",
            "‚úÖ Uploaded tab: Snooker Daily\n",
            "‚úÖ Uploaded tab: Ice Hockey Daily\n",
            "‚úÖ Uploaded tab: Horse Racing Daily\n",
            "‚úÖ Uploaded tab: Golf Daily\n",
            "‚úÖ Uploaded tab: Politics Daily\n",
            "‚úÖ Uploaded tab: Tennis Daily\n",
            "‚úÖ Uploaded tab: Greyhound Racing Daily\n",
            "‚úÖ Uploaded tab: Football Daily\n",
            "‚úÖ Uploaded tab: Motor Sport Daily\n",
            "‚úÖ Uploaded tab: Cricket Daily\n",
            "‚úÖ Uploaded tab: Darts Daily\n",
            "‚úÖ Uploaded tab: Basketball Daily\n",
            "‚úÖ Uploaded tab: American Football Daily\n",
            "‚úÖ Uploaded tab: Rugby Union Daily\n",
            "‚úÖ Uploaded tab: Cycling Daily\n",
            "‚úÖ Uploaded tab: Gaelic Games Daily\n",
            "‚úÖ Uploaded tab: Rolling Horse Racing\n",
            "‚úÖ Uploaded tab: Rolling Greyhound Racing\n",
            "‚úÖ Dashboard KPIs updated (no resize)\n"
          ]
        }
      ]
    }
  ]
}