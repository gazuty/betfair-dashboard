{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tbV1No038w0xJfNjEoodAMRm4uZdXZEj",
      "authorship_tag": "ABX9TyOjIxg6A1xozMlZvIFmEV6/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gazuty/betfair-dashboard/blob/main/Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Betfair Dashboard\n",
        "# Built by Gazuty (c) 2025\n",
        "# This notebook processes Betfair profit/loss data, builds analytics tables, and publishes outputs to Google Sheets.\n",
        "\n",
        "# --- STEP 0: Configuration ---\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Your Drive / folder paths ‚îÄ‚îÄ‚îÄ\n",
        "BASE_FOLDER       = '/content/drive/My Drive/Betfair'\n",
        "MASTER_CSV        = os.path.join(BASE_FOLDER, 'Betfair_Master.csv')\n",
        "ARCHIVE_FOLDER    = os.path.join(BASE_FOLDER, 'Archive')\n",
        "BETTING_PATTERN   = os.path.join(BASE_FOLDER, 'BettingPandL*.csv')\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Google Sheet settings ‚îÄ‚îÄ‚îÄ\n",
        "GOOGLE_SHEET_NAME = 'Betfair Dashboard'\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Business rules ‚îÄ‚îÄ‚îÄ\n",
        "VALID_SPORTS      = ['Horse Racing', 'Greyhound Racing']\n",
        "MIN_STRIKE_BETS   = 50\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Setup ‚îÄ‚îÄ‚îÄ\n",
        "os.makedirs(ARCHIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "print(\"\\u2705 Configuration loaded:\")\n",
        "print(f\"  BASE_FOLDER        = {BASE_FOLDER}\")\n",
        "print(f\"  MASTER_CSV         = {MASTER_CSV}\")\n",
        "print(f\"  ARCHIVE_FOLDER     = {ARCHIVE_FOLDER}\")\n",
        "print(f\"  BETTING_PATTERN    = {BETTING_PATTERN}\")\n",
        "print(f\"  GOOGLE_SHEET_NAME  = {GOOGLE_SHEET_NAME}\")\n",
        "print(f\"  VALID_SPORTS       = {VALID_SPORTS}\")\n",
        "print(f\"  MIN_STRIKE_BETS    = {MIN_STRIKE_BETS}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy268ho4Fip2",
        "outputId": "b80ca667-24ed-458c-c557-d9549ed427ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded:\n",
            "  BASE_FOLDER        = /content/drive/My Drive/Betfair\n",
            "  MASTER_CSV         = /content/drive/My Drive/Betfair/Betfair_Master.csv\n",
            "  ARCHIVE_FOLDER     = /content/drive/My Drive/Betfair/Archive\n",
            "  BETTING_PATTERN    = /content/drive/My Drive/Betfair/BettingPandL*.csv\n",
            "  GOOGLE_SHEET_NAME  = Betfair Dashboard\n",
            "  VALID_SPORTS       = ['Horse Racing', 'Greyhound Racing']\n",
            "  MIN_STRIKE_BETS    = 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: Master Updater ---\n",
        "\n",
        "import pandas as pd, glob, shutil\n",
        "\n",
        "REQUIRED_COLS = ['Market', 'Settled date']\n",
        "\n",
        "def update_betfair_master():\n",
        "    print(\"\\U0001F504 Starting master update\")\n",
        "\n",
        "    # 1‚É£ Load or initialize master\n",
        "    if os.path.exists(MASTER_CSV):\n",
        "        df_master = pd.read_csv(MASTER_CSV)\n",
        "        df_master['Settled date'] = pd.to_datetime(df_master['Settled date'], errors='coerce')\n",
        "        df_master['Profit_Loss'] = pd.to_numeric(df_master['Profit_Loss'], errors='coerce')\n",
        "        df_master = df_master.dropna(subset=['Settled date']).reset_index(drop=True)\n",
        "        print(f\"\\u2705 Loaded master ({len(df_master)} rows)\")\n",
        "    else:\n",
        "        print(\"\\u26a0 No existing master found ‚Äî starting fresh\")\n",
        "        df_master = pd.DataFrame(columns=REQUIRED_COLS + ['Profit_Loss'])\n",
        "\n",
        "    # 2‚É£ Gather raw files\n",
        "    raw_files = glob.glob(BETTING_PATTERN)\n",
        "    print(f\"üìÇ Found {len(raw_files)} raw file(s)\")\n",
        "\n",
        "    if not raw_files:\n",
        "        print(\"\\u26a0 No raw files to process ‚Äî exiting.\")\n",
        "        return\n",
        "\n",
        "    # 3‚É£ Process each file\n",
        "    dfs = []\n",
        "    for filepath in raw_files:\n",
        "        fname = os.path.basename(filepath)\n",
        "        print(f\"üìÖ {fname}\", end=\"\")\n",
        "\n",
        "        df = pd.read_csv(filepath)\n",
        "        missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
        "        if missing:\n",
        "            print(f\" ‚Üí ‚ùå missing columns {missing}\")\n",
        "            continue\n",
        "\n",
        "        profs = [c for c in df.columns if 'profit' in c.lower()]\n",
        "        if not profs:\n",
        "            print(\" ‚Üí ‚ùå no profit column found\")\n",
        "            continue\n",
        "\n",
        "        pick = next((c for c in profs if 'aud' in c.lower()), profs[0])\n",
        "        df['Profit_Loss'] = pd.to_numeric(df[pick], errors='coerce')\n",
        "        df['Settled date'] = pd.to_datetime(df['Settled date'], errors='coerce')\n",
        "        df = df[['Market', 'Settled date', 'Profit_Loss']].dropna(subset=['Settled date'])\n",
        "\n",
        "        dfs.append(df)\n",
        "        print(f\" ‚Üí {len(df)} rows from '{pick}'\")\n",
        "\n",
        "    if not dfs:\n",
        "        print(\"\\u26a0 No valid data loaded from raw files ‚Äî exiting.\")\n",
        "        return\n",
        "\n",
        "    # 4‚É£ Combine and deduplicate\n",
        "    df_new = pd.concat(dfs, ignore_index=True)\n",
        "    df_new = df_new.dropna(subset=['Settled date']).reset_index(drop=True)\n",
        "\n",
        "    df_master['_key'] = (\n",
        "        df_master['Market'].astype(str) + \"|\" +\n",
        "        df_master['Settled date'].dt.strftime('%Y-%m-%d %H:%M:%S') + \"|\" +\n",
        "        df_master['Profit_Loss'].astype(str)\n",
        "    )\n",
        "    df_new['_key'] = (\n",
        "        df_new['Market'].astype(str) + \"|\" +\n",
        "        df_new['Settled date'].dt.strftime('%Y-%m-%d %H:%M:%S') + \"|\" +\n",
        "        df_new['Profit_Loss'].astype(str)\n",
        "    )\n",
        "\n",
        "    df_unique = df_new[~df_new['_key'].isin(df_master['_key'])]\n",
        "    print(f\"\\u2705 {len(df_unique)} unique new row(s) identified\")\n",
        "\n",
        "    # 5‚É£ Merge and save\n",
        "    if not df_unique.empty:\n",
        "        df_combined = pd.concat([\n",
        "            df_master.drop(columns=['_key']),\n",
        "            df_unique.drop(columns=['_key'])\n",
        "        ], ignore_index=True)\n",
        "        df_combined.to_csv(MASTER_CSV, index=False)\n",
        "        print(f\"\\u2705 Master updated ({len(df_combined)} rows) ‚Üí {MASTER_CSV}\")\n",
        "    else:\n",
        "        print(\"\\u26a0 No new rows to add ‚Äî master unchanged.\")\n",
        "\n",
        "    # 6‚É£ Archive files\n",
        "    for filepath in raw_files:\n",
        "        fname = os.path.basename(filepath)\n",
        "        shutil.move(filepath, os.path.join(ARCHIVE_FOLDER, fname))\n",
        "        print(f\"üì¶ Archived {fname}\")\n",
        "\n",
        "# Run the function\n",
        "update_betfair_master()\n"
      ],
      "metadata": {
        "id": "6Bgp_pFfIjKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72e5272-819b-4b23-cf70-dd10f5ecfeb0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Starting master update\n",
            "‚ö† No existing master found ‚Äî starting fresh\n",
            "üìÇ Found 0 raw file(s)\n",
            "‚ö† No raw files to process ‚Äî exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: Load Master ---\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"üìÇ Loading master from: {MASTER_CSV}\")\n",
        "df = pd.read_csv(MASTER_CSV)\n",
        "\n",
        "# Ensure correct dtypes\n",
        "df['Settled date'] = pd.to_datetime(df['Settled date'], errors='coerce')\n",
        "df['Profit_Loss'] = pd.to_numeric(df['Profit_Loss'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid dates\n",
        "before = len(df)\n",
        "df = df.dropna(subset=['Settled date']).reset_index(drop=True)\n",
        "after = len(df)\n",
        "\n",
        "print(f\"‚úÖ {after} rows loaded (dropped {before - after} invalid dates).\")\n",
        "print(f\"   Profit_Loss dtype: {df['Profit_Loss'].dtype}\")\n"
      ],
      "metadata": {
        "id": "Ag8x2FTv98Bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "bedfe113-3ba6-414a-95e9-25afce82b341"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading master from: /content/drive/My Drive/Betfair/Betfair_Master.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Betfair/Betfair_Master.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3423822050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÇ Loading master from: {MASTER_CSV}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMASTER_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Ensure correct dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Betfair/Betfair_Master.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: Feature Extraction ---\n",
        "\n",
        "# 1Ô∏è‚É£ Extract Sport from Market (first token before slash)\n",
        "df['Sport'] = df['Market'].str.extract(r'^([^/]+)/')[0].str.strip()\n",
        "\n",
        "# 2Ô∏è‚É£ Extract Track_Info and Event_Description for racing sports\n",
        "racing_mask = df['Sport'].isin(VALID_SPORTS)\n",
        "track_event = df.loc[racing_mask, 'Market'].str.extract(r'/\\s*(.*?)\\s*:\\s*(.*)')\n",
        "track_event.columns = ['Track_Info', 'Event_Description']\n",
        "df.loc[racing_mask, ['Track_Info', 'Event_Description']] = track_event\n",
        "\n",
        "# 3Ô∏è‚É£ Extract Country from parentheses in Track_Info\n",
        "df['Country'] = df['Track_Info'].str.extract(r'\\(([^)]+)\\)')[0]\n",
        "\n",
        "# 4Ô∏è‚É£ Fill missing country values\n",
        "df['Country'] = df['Country'].fillna('UK')\n",
        "df.loc[~df['Sport'].isin(VALID_SPORTS), 'Country'] = 'Unknown'\n",
        "\n",
        "# 5Ô∏è‚É£ Clean up Track_Info to produce Track_Name (remove dates and country)\n",
        "df['Track_Name'] = (\n",
        "    df['Track_Info']\n",
        "      .str.replace(r'\\([^)]*\\)', '', regex=True)\n",
        "      .str.replace(r'\\b\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+\\b', '', regex=True)\n",
        "      .str.strip()\n",
        ")\n",
        "\n",
        "# 6Ô∏è‚É£ Preview output\n",
        "preview = df.loc[df['Track_Name'].notna(), ['Sport', 'Track_Name', 'Country']].drop_duplicates().head(10)\n",
        "print(\"‚úÖ Feature extraction complete ‚Äî first few tracks:\")\n",
        "print(preview)\n"
      ],
      "metadata": {
        "id": "3h7weeM--HWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: Build summary tables (daily, weekly, monthly, sport, country) ---\n",
        "\n",
        "print(\"üîß STEP 4: Building summary tables...\")\n",
        "\n",
        "# 4.1Ô∏è‚É£ Daily Summary (chronological)\n",
        "by_day = (\n",
        "    df.groupby(df['Settled date'].dt.date)['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index(name='Profit_Loss')\n",
        "      .rename(columns={'Settled date': 'Day'})\n",
        ")\n",
        "by_day = by_day.sort_values('Day').reset_index(drop=True)\n",
        "by_day['Cumulative_Profit_Loss'] = by_day['Profit_Loss'].cumsum()\n",
        "by_day[['Profit_Loss', 'Cumulative_Profit_Loss']] = by_day[['Profit_Loss', 'Cumulative_Profit_Loss']].round(2)\n",
        "\n",
        "# 4.2Ô∏è‚É£ Rolling Returns (start from 1 March)\n",
        "by_day['Day'] = pd.to_datetime(by_day['Day'])\n",
        "rolling_start = pd.to_datetime('2025-03-01')\n",
        "rolling_df = by_day[by_day['Day'] >= rolling_start].copy()\n",
        "rolling_df = rolling_df.set_index('Day')\n",
        "\n",
        "rolling_df['Rolling 2w'] = by_day.set_index('Day')['Profit_Loss'].rolling(window='14D').sum()\n",
        "rolling_df['Rolling 4w'] = by_day.set_index('Day')['Profit_Loss'].rolling(window='28D').sum()\n",
        "rolling_df['Rolling 8w'] = by_day.set_index('Day')['Profit_Loss'].rolling(window='56D').sum()\n",
        "\n",
        "rolling_df = rolling_df.drop(columns=['Profit_Loss', 'Cumulative_Profit_Loss']).reset_index()\n",
        "rolling_df = rolling_df.round(2)\n",
        "rolling_df.columns = ['Day', 'Rolling 2w', 'Rolling 4w', 'Rolling 8w']\n",
        "\n",
        "# 4.3Ô∏è‚É£ Weekly Summary (week starts Sunday)\n",
        "by_week = (\n",
        "    df.set_index('Settled date')\n",
        "      .resample('W-SUN')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .rename(columns={'Settled date': 'Week Starting'})\n",
        ")\n",
        "by_week['Profit_Loss'] = by_week['Profit_Loss'].round(2)\n",
        "\n",
        "# 4.4Ô∏è‚É£ Rolling by Sport (Horse Racing & Greyhounds)\n",
        "rolling_by_sport = {}\n",
        "for sport in ['Horse Racing', 'Greyhound Racing']:\n",
        "    sport_df = df[df['Sport'] == sport].copy()\n",
        "    sport_by_day = (\n",
        "        sport_df.groupby(sport_df['Settled date'].dt.date)['Profit_Loss']\n",
        "        .sum()\n",
        "        .reset_index(name='Profit_Loss')\n",
        "        .rename(columns={'Settled date': 'Day'})\n",
        "        .sort_values('Day')\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    sport_by_day['Day'] = pd.to_datetime(sport_by_day['Day'])\n",
        "    sport_by_day = sport_by_day.set_index('Day')\n",
        "\n",
        "    result = sport_by_day.copy()\n",
        "    result['Rolling 2w'] = sport_by_day['Profit_Loss'].rolling(window='14D').sum()\n",
        "    result['Rolling 4w'] = sport_by_day['Profit_Loss'].rolling(window='28D').sum()\n",
        "    result['Rolling 8w'] = sport_by_day['Profit_Loss'].rolling(window='56D').sum()\n",
        "\n",
        "    result = result.drop(columns=['Profit_Loss']).reset_index()\n",
        "    result = result[result['Day'] >= rolling_start].round(2)\n",
        "    result.columns = ['Day', 'Rolling 2w', 'Rolling 4w', 'Rolling 8w']\n",
        "    rolling_by_sport[sport] = result\n",
        "\n",
        "# 4.5Ô∏è‚É£ Monthly Summary ‚Äî using 'ME' to avoid deprecation warning\n",
        "by_month = (\n",
        "    df.set_index('Settled date')\n",
        "      .resample('ME')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        ")\n",
        "by_month['Month'] = by_month['Settled date'].dt.to_period('M').astype(str)\n",
        "by_month = by_month[['Month', 'Profit_Loss']]\n",
        "by_month['Profit_Loss'] = by_month['Profit_Loss'].round(2)\n",
        "\n",
        "# 4.6Ô∏è‚É£ Sport Summary\n",
        "by_sport = (\n",
        "    df.groupby('Sport')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .round({'Profit_Loss': 2})\n",
        ")\n",
        "\n",
        "# 4.7Ô∏è‚É£ Country Summary\n",
        "by_country = (\n",
        "    df.groupby('Country')['Profit_Loss']\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .round({'Profit_Loss': 2})\n",
        ")\n",
        "\n",
        "# 4.8Ô∏è‚É£ Daily Summaries per Sport (with cumulative P/L)\n",
        "sport_daily = {}\n",
        "for sport in df['Sport'].dropna().unique():\n",
        "    temp = (\n",
        "        df[df['Sport'] == sport]\n",
        "          .groupby(df['Settled date'].dt.date)['Profit_Loss']\n",
        "          .sum()\n",
        "          .reset_index(name='Profit_Loss')\n",
        "          .rename(columns={'Settled date': 'Day'})\n",
        "          .sort_values('Day')\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    temp['Cumulative_Profit_Loss'] = temp['Profit_Loss'].cumsum().round(2)\n",
        "    temp['Profit_Loss'] = temp['Profit_Loss'].round(2)\n",
        "    sport_daily[f\"{sport} Daily\"] = temp\n",
        "\n",
        "# ‚úÖ Summary Checks\n",
        "print(f\"‚úÖ By Day: {len(by_day)} rows (last: {by_day['Day'].max().date()})\")\n",
        "print(f\"‚úÖ Rolling Returns: {len(rolling_df)} rows (last: {rolling_df['Day'].max().date()})\")\n",
        "for k, v in rolling_by_sport.items():\n",
        "    print(f\"‚úÖ Rolling {k}: {len(v)} rows (last: {v['Day'].max().date()})\")\n",
        "print(f\"‚úÖ By Week: {len(by_week)} rows (last: {by_week['Week Starting'].max().date()})\")\n",
        "print(f\"‚úÖ By Month: {len(by_month)} rows (last: {by_month['Month'].max()})\")\n",
        "print(f\"‚úÖ By Sport: {len(by_sport)} sports ‚Üí {by_sport['Sport'].tolist()}\")\n",
        "print(f\"‚úÖ By Country: {len(by_country)} countries ‚Üí {by_country['Country'].tolist()}\")\n"
      ],
      "metadata": {
        "id": "V_K4tw9-cWvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: Track Summaries ---\n",
        "\n",
        "# 1Ô∏è‚É£ Aggregate P/L per track for Horse and Greyhound Racing\n",
        "track_df = (\n",
        "    df[df['Sport'].isin(VALID_SPORTS)]\n",
        "      .groupby(['Sport', 'Track_Name'], as_index=False)['Profit_Loss']\n",
        "      .sum()\n",
        ")\n",
        "track_df['Profit_Loss'] = track_df['Profit_Loss'].round(2)\n",
        "\n",
        "# 2Ô∏è‚É£ Create summary groups\n",
        "tracks = {\n",
        "    'Track Stats':               track_df,\n",
        "    'Top Horse Tracks':          track_df.query(\"Sport == 'Horse Racing'\").nlargest(15, 'Profit_Loss'),\n",
        "    'Bottom Horse Tracks':       track_df.query(\"Sport == 'Horse Racing'\").nsmallest(15, 'Profit_Loss'),\n",
        "    'Top Greyhound Tracks':      track_df.query(\"Sport == 'Greyhound Racing'\").nlargest(15, 'Profit_Loss'),\n",
        "    'Bottom Greyhound Tracks':   track_df.query(\"Sport == 'Greyhound Racing'\").nsmallest(15, 'Profit_Loss'),\n",
        "}\n",
        "\n",
        "# 3Ô∏è‚É£ Preview sample\n",
        "print(\"‚úÖ Track summaries built.\")\n",
        "print(\" ‚Ä¢ Sample Track Stats:\")\n",
        "print(track_df.head())\n",
        "print(\" ‚Ä¢ Top Horse Tracks:\")\n",
        "print(tracks['Top Horse Tracks'][['Track_Name', 'Profit_Loss']].head())\n"
      ],
      "metadata": {
        "id": "nFAP0CNsccL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: Strike Rates ---\n",
        "\n",
        "# 1Ô∏è‚É£ Filter to Horse & Greyhound Racing\n",
        "df_racing = df[df['Sport'].isin(VALID_SPORTS)].copy()\n",
        "\n",
        "# 2Ô∏è‚É£ Compute total bets and wins per track\n",
        "strike_df = (\n",
        "    df_racing\n",
        "      .groupby(['Sport', 'Track_Name'])['Profit_Loss']\n",
        "      .agg(\n",
        "          total_bets='count',\n",
        "          wins=lambda x: (x > 0).sum()\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Calculate strike rate\n",
        "strike_df['Strike_Rate'] = (strike_df['wins'] / strike_df['total_bets']).round(4)\n",
        "\n",
        "# 4Ô∏è‚É£ Filter by minimum bets threshold\n",
        "strike_df_filtered = strike_df[strike_df['total_bets'] >= MIN_STRIKE_BETS].reset_index(drop=True)\n",
        "\n",
        "# 5Ô∏è‚É£ Extract Top & Bottom Strike Rate Tracks\n",
        "top_strike    = strike_df_filtered.nlargest(10, 'Strike_Rate').reset_index(drop=True)\n",
        "bottom_strike = strike_df_filtered.nsmallest(10, 'Strike_Rate').reset_index(drop=True)\n",
        "\n",
        "# 6Ô∏è‚É£ Preview\n",
        "print(f\"‚úÖ Strike rates computed (min {MIN_STRIKE_BETS} bets):\")\n",
        "print(\"Top 10 Strike Rates:\")\n",
        "print(top_strike[['Sport', 'Track_Name', 'total_bets', 'wins', 'Strike_Rate']])\n",
        "print(\"\\nBottom 10 Strike Rates:\")\n",
        "print(bottom_strike[['Sport', 'Track_Name', 'total_bets', 'wins', 'Strike_Rate']])\n"
      ],
      "metadata": {
        "id": "YrBPZlTccebQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7: Prepare all_sheets for export ---\n",
        "\n",
        "# 7.1Ô∏è‚É£ Ensure all_sheets exists\n",
        "if 'all_sheets' not in locals():\n",
        "    all_sheets = {}\n",
        "\n",
        "# 7.2Ô∏è‚É£ Add core summaries\n",
        "all_sheets.update({\n",
        "    'By Day':           by_day,\n",
        "    'By Day Sorted':    by_day.sort_values('Profit_Loss', ascending=False).reset_index(drop=True),\n",
        "    'By Week':          by_week,\n",
        "    'Cumulative':       by_day[['Day', 'Cumulative_Profit_Loss']].rename(columns={'Cumulative_Profit_Loss': 'Cumulative'}),\n",
        "    'By Month':         by_month,\n",
        "    'By Sport':         by_sport,\n",
        "    'By Country':       by_country,\n",
        "    'Rolling Returns':  rolling_df\n",
        "})\n",
        "\n",
        "# 7.3Ô∏è‚É£ Add track-level summaries\n",
        "all_sheets.update({\n",
        "    'Track Stats':             tracks['Track Stats'],\n",
        "    'Top Horse Tracks':        tracks['Top Horse Tracks'],\n",
        "    'Bottom Horse Tracks':     tracks['Bottom Horse Tracks'],\n",
        "    'Top Greyhound Tracks':    tracks['Top Greyhound Tracks'],\n",
        "    'Bottom Greyhound Tracks': tracks['Bottom Greyhound Tracks'],\n",
        "})\n",
        "\n",
        "# 7.4Ô∏è‚É£ Add strike rate summaries\n",
        "all_sheets.update({\n",
        "    'Top Strike Rates':    top_strike,\n",
        "    'Bottom Strike Rates': bottom_strike,\n",
        "})\n",
        "\n",
        "# 7.5Ô∏è‚É£ Add daily summaries for each sport\n",
        "all_sheets.update(sport_daily)\n",
        "\n",
        "# 7.6Ô∏è‚É£ Add rolling returns by sport\n",
        "for sport, df_rolling in rolling_by_sport.items():\n",
        "    sheet_name = f\"Rolling {sport}\"\n",
        "    all_sheets[sheet_name] = df_rolling\n",
        "\n",
        "# 7.7Ô∏è‚É£ Final review of included sheets\n",
        "print(f\"‚úÖ Prepared {len(all_sheets)} tables for export:\")\n",
        "for name in all_sheets:\n",
        "    print(f\"  ‚Ä¢ {name}\")\n"
      ],
      "metadata": {
        "id": "OP-sN4D9cgqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä Top Horse Tracks preview:\")\n",
        "print(tracks['Top Horse Tracks'].head())\n",
        "print(\"üìä Bottom Horse Tracks preview:\")\n",
        "print(tracks['Bottom Horse Tracks'].head())\n"
      ],
      "metadata": {
        "id": "NfU0nlBeNY4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 8: Export to Google Sheets ---\n",
        "\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from datetime import date\n",
        "\n",
        "# 1Ô∏è‚É£ Authenticate and connect\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# 2Ô∏è‚É£ Open Google Sheet\n",
        "sh = next((s for s in gc.openall() if s.title == GOOGLE_SHEET_NAME), None)\n",
        "if not sh:\n",
        "    raise Exception(f\"‚ùå Sheet not found: {GOOGLE_SHEET_NAME}\")\n",
        "print(f\"‚úÖ Connected to '{GOOGLE_SHEET_NAME}'\")\n",
        "\n",
        "# 3Ô∏è‚É£ Upload each table\n",
        "for name, df_out in all_sheets.items():\n",
        "    # Format Profit_Loss\n",
        "    if 'Profit_Loss' in df_out.columns:\n",
        "        df_out['Profit_Loss'] = pd.to_numeric(df_out['Profit_Loss'], errors='coerce').round(2)\n",
        "        df_out['Profit_Loss'] = df_out['Profit_Loss'].map(lambda x: f\"{x:.2f}\" if pd.notnull(x) else \"\")\n",
        "\n",
        "    # Format week date\n",
        "    if 'Week Starting' in df_out.columns:\n",
        "        df_out['Week Starting'] = df_out['Week Starting'].astype(str)\n",
        "\n",
        "    # Round other numeric columns\n",
        "    for col in df_out.select_dtypes(include=['float', 'int']).columns:\n",
        "        df_out[col] = df_out[col].round(2)\n",
        "\n",
        "    # Upload to sheet\n",
        "    try:\n",
        "        ws = sh.worksheet(name)\n",
        "        ws.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=name, rows=1000, cols=20)\n",
        "\n",
        "    set_with_dataframe(ws, df_out)\n",
        "    print(f\"‚úÖ Uploaded tab: {name}\")\n",
        "\n",
        "# 4Ô∏è‚É£ Update KPI dashboard\n",
        "try:\n",
        "    dash = sh.worksheet('Dashboard')\n",
        "    dash.clear()\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    dash = sh.add_worksheet('Dashboard', rows=10, cols=5)\n",
        "\n",
        "total_profit = round(df['Profit_Loss'].sum(), 2)\n",
        "total_bets   = len(df)\n",
        "best_day     = df.groupby(df['Settled date'].dt.date)['Profit_Loss'].sum().idxmax()\n",
        "worst_day    = df.groupby(df['Settled date'].dt.date)['Profit_Loss'].sum().idxmin()\n",
        "\n",
        "kpis = [\n",
        "    ['Metric', 'Value'],\n",
        "    ['Total Profit/Loss', total_profit],\n",
        "    ['Number of Bets', total_bets],\n",
        "    ['Best Day', str(best_day)],\n",
        "    ['Worst Day', str(worst_day)],\n",
        "    ['Generated on', str(date.today())]\n",
        "]\n",
        "dash.update(values=kpis, range_name='A1')\n",
        "\n",
        "print(\"‚úÖ Dashboard KPIs updated\")\n"
      ],
      "metadata": {
        "id": "sw42u_CuCOsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_sII6iunFgwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}