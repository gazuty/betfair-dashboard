{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1tbV1No038w0xJfNjEoodAMRm4uZdXZEj",
      "authorship_tag": "ABX9TyOCbuslMM/1Q0ULrMUR82vG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gazuty/betfair-dashboard/blob/main/Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: Master Updater ---\n",
        "\n",
        "import os, glob, pandas as pd, shutil\n",
        "\n",
        "def update_betfair_master():\n",
        "    BASE_FOLDER    = '/content/drive/My Drive/Betfair'\n",
        "    ARCHIVE_FOLDER = os.path.join(BASE_FOLDER, 'Archive')\n",
        "    os.makedirs(ARCHIVE_FOLDER, exist_ok=True)\n",
        "\n",
        "    MASTER_CSV     = os.path.join(BASE_FOLDER, 'Betfair_Master.csv')\n",
        "    BETTING_PATTERN= os.path.join(BASE_FOLDER, 'BettingPandL*.csv')\n",
        "    RESULTS_FILE   = os.path.join(BASE_FOLDER, 'Results Summary export 25-05-11 095900.csv')\n",
        "\n",
        "    # 1Ô∏è‚É£ Load or start fresh\n",
        "    if os.path.exists(MASTER_CSV):\n",
        "        df_master = pd.read_csv(MASTER_CSV)\n",
        "        df_master['Settled date'] = pd.to_datetime(df_master['Settled date'], errors='coerce')\n",
        "        df_master['Profit_Loss']   = pd.to_numeric(df_master['Profit_Loss'], errors='coerce')\n",
        "        print(f\"‚úÖ Loaded master: {MASTER_CSV} ({len(df_master)} rows)\")\n",
        "    else:\n",
        "        print(\"‚ö† No master found ‚Äî starting fresh.\")\n",
        "        df_master = pd.DataFrame()\n",
        "\n",
        "    # 2Ô∏è‚É£ Find all new files\n",
        "    new_files = glob.glob(BETTING_PATTERN)\n",
        "    if os.path.exists(RESULTS_FILE):\n",
        "        new_files.append(RESULTS_FILE)\n",
        "    print(f\"üìÇ Found {len(new_files)} new file(s).\")\n",
        "    if not new_files:\n",
        "        return\n",
        "\n",
        "    # 3Ô∏è‚É£ Load & detect profit column\n",
        "    dfs_new = []\n",
        "    for file in new_files:\n",
        "        print(f\"üì• Processing {os.path.basename(file)}\")\n",
        "        df = pd.read_csv(file)\n",
        "        # pick any column containing ‚Äúprofit‚Äù\n",
        "        profs = [c for c in df.columns if 'profit' in c.lower()]\n",
        "        if not profs:\n",
        "            print(f\"‚ö† Skipping ‚Äî no profit column in {os.path.basename(file)}\")\n",
        "            continue\n",
        "        # prefer AUD if present\n",
        "        pick = next((c for c in profs if 'aud' in c.lower()), profs[0])\n",
        "        df['Profit_Loss'] = pd.to_numeric(df[pick], errors='coerce')\n",
        "        dfs_new.append(df)\n",
        "        print(f\"  ‚Ü≥ used '{pick}' as Profit_Loss\")\n",
        "\n",
        "    if not dfs_new:\n",
        "        print(\"‚ö† No valid data to merge.\")\n",
        "        return\n",
        "\n",
        "    # 4Ô∏è‚É£ Concat + clean dates\n",
        "    df_new = pd.concat(dfs_new, ignore_index=True)\n",
        "    df_new['Settled date'] = pd.to_datetime(df_new['Settled date'], errors='coerce')\n",
        "    df_new.dropna(subset=['Settled date'], inplace=True)\n",
        "\n",
        "    # 5Ô∏è‚É£ Dedupe via composite key\n",
        "    df_master['key'] = (\n",
        "        df_master['Market'].astype(str) + \"|\" +\n",
        "        df_master['Settled date'].dt.strftime('%Y-%m-%d %H:%M:%S') + \"|\" +\n",
        "        df_master['Profit_Loss'].astype(str)\n",
        "    )\n",
        "    df_new['key'] = (\n",
        "        df_new['Market'].astype(str) + \"|\" +\n",
        "        df_new['Settled date'].dt.strftime('%Y-%m-%d %H:%M:%S') + \"|\" +\n",
        "        df_new['Profit_Loss'].astype(str)\n",
        "    )\n",
        "    df_unique = df_new[~df_new['key'].isin(df_master.get('key', []))]\n",
        "    print(f\"‚úÖ {len(df_unique)} new row(s) to add.\")\n",
        "\n",
        "    # 6Ô∏è‚É£ Merge & save\n",
        "    if not df_unique.empty:\n",
        "        df_combined = pd.concat([df_master.drop(columns=['key'], errors='ignore'),\n",
        "                                  df_unique.drop(columns=['key'])],\n",
        "                                  ignore_index=True)\n",
        "        df_combined.to_csv(MASTER_CSV, index=False)\n",
        "        print(f\"‚úÖ Master updated ‚Üí {len(df_combined)} rows\")\n",
        "    else:\n",
        "        print(\"‚ö† No additions required.\")\n",
        "\n",
        "    # 7Ô∏è‚É£ Archive processed\n",
        "    for f in new_files:\n",
        "        shutil.move(f, os.path.join(ARCHIVE_FOLDER, os.path.basename(f)))\n",
        "        print(f\"üì¶ Archived {os.path.basename(f)}\")\n",
        "\n",
        "# Run it\n",
        "update_betfair_master()\n"
      ],
      "metadata": {
        "id": "6Bgp_pFfIjKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b145d9-0899-4912-ce23-4ffa4f4b1299"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded master: /content/drive/My Drive/Betfair/Betfair_Master.csv (18886 rows)\n",
            "üìÇ Found 0 new file(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: Load Master ---\n",
        "\n",
        "import os, pandas as pd\n",
        "\n",
        "BASE_FOLDER = '/content/drive/My Drive/Betfair'\n",
        "MASTER_CSV  = os.path.join(BASE_FOLDER, 'Betfair_Master.csv')\n",
        "\n",
        "print(f\"Loading master from {MASTER_CSV}\")\n",
        "df = pd.read_csv(MASTER_CSV)\n",
        "df['Settled date'] = pd.to_datetime(df['Settled date'], errors='coerce')\n",
        "df['Profit_Loss']   = pd.to_numeric(df['Profit_Loss'], errors='coerce')\n",
        "df.dropna(subset=['Settled date'], inplace=True)\n",
        "\n",
        "print(f\"‚úÖ {len(df)} rows loaded; Profit_Loss: {df['Profit_Loss'].dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag8x2FTv98Bd",
        "outputId": "2426a6ad-9d48-4a94-9b52-687311006c2f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading master from /content/drive/My Drive/Betfair/Betfair_Master.csv\n",
            "‚úÖ 18886 rows loaded; Profit_Loss: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: Extract Sport, clean Track_Name, and Country ---\n",
        "\n",
        "import re\n",
        "\n",
        "df['Sport'] = df['Market'].str.extract(r'^([^/]+)/')[0].str.strip()\n",
        "racing_df = df[df['Sport'].isin(['Horse Racing', 'Greyhound Racing'])].copy()\n",
        "racing_df[['Track_Info', 'Event_Description']] = racing_df['Market'].str.extract(r'/\\s*(.*?)\\s*:\\s*(.*)')\n",
        "\n",
        "def extract_track_and_country(track_info):\n",
        "    if pd.isna(track_info):\n",
        "        return pd.Series([None, 'Unknown'])\n",
        "    if '(' in track_info and ')' in track_info:\n",
        "        inside = track_info.split('(')[1].replace(')', '').strip()\n",
        "        country = inside.split()[0]\n",
        "        track = track_info.split('(')[0].strip()\n",
        "    else:\n",
        "        track = track_info.strip()\n",
        "        country = 'Unknown'\n",
        "    return pd.Series([track, country])\n",
        "\n",
        "def clean_track_name(track):\n",
        "    if pd.isna(track):\n",
        "        return None\n",
        "    return re.sub(r'\\b\\d{1,2}(st|nd|rd|th)?\\s\\w+\\b', '', track).strip()\n",
        "\n",
        "racing_df[['Track_Name_Raw', 'Country']] = racing_df['Track_Info'].apply(extract_track_and_country)\n",
        "racing_df['Track_Name'] = racing_df['Track_Name_Raw'].apply(clean_track_name)\n",
        "df = df.merge(racing_df[['Market', 'Track_Name', 'Country']], on='Market', how='left')\n",
        "print(f\"‚úÖ After feature extraction: {len(df)} rows, Profit_Loss dtype: {df['Profit_Loss'].dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h7weeM--HWz",
        "outputId": "676c82a0-29c0-4477-eac4-bba952d755b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ After feature extraction: 20838 rows, Profit_Loss dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: Build complete summary tables (daily, cumulative, weekly, monthly, sport, country) ---\n",
        "\n",
        "df['Day'] = df['Settled date'].dt.date\n",
        "df['Month'] = df['Settled date'].dt.to_period('M').astype(str)\n",
        "df['Week Starting'] = (\n",
        "    df['Settled date'].dt.floor('D') -\n",
        "    pd.to_timedelta(df['Settled date'].dt.weekday, unit='d')\n",
        ")\n",
        "\n",
        "by_day = df.groupby('Day')['Profit_Loss'].sum().reset_index()\n",
        "by_day = by_day.sort_values('Day').reset_index(drop=True)\n",
        "by_day['Cumulative_Profit_Loss'] = by_day['Profit_Loss'].cumsum()\n",
        "by_day['Profit_Loss'] = pd.to_numeric(by_day['Profit_Loss'], errors='coerce').round(2)\n",
        "by_day['Cumulative_Profit_Loss'] = pd.to_numeric(by_day['Cumulative_Profit_Loss'], errors='coerce').round(2)\n",
        "\n",
        "by_week = df.groupby('Week Starting')['Profit_Loss'].sum().reset_index()\n",
        "by_week = by_week.sort_values('Week Starting').reset_index(drop=True)\n",
        "by_week['Profit_Loss'] = pd.to_numeric(by_week['Profit_Loss'], errors='coerce').round(2)\n",
        "\n",
        "by_month = df.groupby('Month')['Profit_Loss'].sum().reset_index()\n",
        "by_month['Profit_Loss'] = pd.to_numeric(by_month['Profit_Loss'], errors='coerce').round(2)\n",
        "\n",
        "by_sport = df.groupby('Sport')['Profit_Loss'].sum().reset_index()\n",
        "by_sport['Profit_Loss'] = pd.to_numeric(by_sport['Profit_Loss'], errors='coerce').round(2)\n",
        "\n",
        "by_country = df.groupby('Country')['Profit_Loss'].sum().reset_index()\n",
        "by_country['Profit_Loss'] = pd.to_numeric(by_country['Profit_Loss'], errors='coerce').round(2)\n",
        "\n",
        "sport_daily = {}\n",
        "for sport in df['Sport'].dropna().unique():\n",
        "    temp = df[df['Sport'] == sport].groupby('Day')['Profit_Loss'].sum().reset_index()\n",
        "    temp = temp.sort_values('Day').reset_index(drop=True)\n",
        "    temp['Cumulative_Profit_Loss'] = temp['Profit_Loss'].cumsum()\n",
        "    temp['Profit_Loss'] = pd.to_numeric(temp['Profit_Loss'], errors='coerce').round(2)\n",
        "    temp['Cumulative_Profit_Loss'] = pd.to_numeric(temp['Cumulative_Profit_Loss'], errors='coerce').round(2)\n",
        "    sport_daily[f\"{sport} Daily\"] = temp\n",
        "\n",
        "# Terminal output for validation\n",
        "print(f\"‚úÖ By Day rows: {len(by_day)}, dtype: {by_day['Profit_Loss'].dtype}\")\n",
        "print(f\"‚úÖ By Week rows: {len(by_week)}, dtype: {by_week['Profit_Loss'].dtype}\")\n",
        "print(f\"‚úÖ By Month rows: {len(by_month)}, dtype: {by_month['Profit_Loss'].dtype}\")\n",
        "print(f\"‚úÖ By Sport rows: {len(by_sport)}, dtype: {by_sport['Profit_Loss'].dtype}\")\n",
        "print(f\"‚úÖ By Country rows: {len(by_country)}, dtype: {by_country['Profit_Loss'].dtype}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_K4tw9-cWvw",
        "outputId": "dc1d1463-4580-41f8-89bd-7ae67bf057b2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ By Day rows: 193, dtype: float64\n",
            "‚úÖ By Week rows: 28, dtype: float64\n",
            "‚úÖ By Month rows: 7, dtype: float64\n",
            "‚úÖ By Sport rows: 14, dtype: float64\n",
            "‚úÖ By Country rows: 8, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: Track summaries ---\n",
        "track_df = df[df['Sport'].isin(['Horse Racing', 'Greyhound Racing'])] \\\n",
        "    .groupby(['Sport', 'Track_Name'])['Profit_Loss'].sum().reset_index()\n",
        "\n",
        "track_df['Profit_Loss'] = track_df['Profit_Loss'].round(2)\n",
        "\n",
        "tracks = {\n",
        "    'Top Horse Tracks': track_df.query(\"Sport == 'Horse Racing'\").nlargest(15, 'Profit_Loss'),\n",
        "    'Bottom Horse Tracks': track_df.query(\"Sport == 'Horse Racing'\").nsmallest(15, 'Profit_Loss'),\n",
        "    'Top Greyhound Tracks': track_df.query(\"Sport == 'Greyhound Racing'\").nlargest(15, 'Profit_Loss'),\n",
        "    'Bottom Greyhound Tracks': track_df.query(\"Sport == 'Greyhound Racing'\").nsmallest(15, 'Profit_Loss')\n",
        "}\n",
        "\n",
        "track_stats = track_df\n",
        "print(\"‚úÖ Track summaries built.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFAP0CNsccL9",
        "outputId": "ab35a11b-438c-48ea-c5d2-31478acfdf51"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Track summaries built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7: Compute strike rates for Horse Racing and Greyhound Racing with min 50 bets ---\n",
        "\n",
        "# Filter for racing sports\n",
        "df_racing = df[df['Sport'].isin(['Horse Racing', 'Greyhound Racing'])].copy()\n",
        "\n",
        "# Group and compute\n",
        "strike_df = (\n",
        "    df_racing.groupby(['Sport', 'Track_Name'])['Profit_Loss']\n",
        "    .agg(\n",
        "        total_bets='count',\n",
        "        wins=lambda x: (x > 0).sum()\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Calculate strike rate\n",
        "strike_df['Strike_Rate'] = strike_df['wins'] / strike_df['total_bets']\n",
        "\n",
        "# Filter for min 50 bets\n",
        "strike_df_filtered = strike_df[strike_df['total_bets'] >= 50]\n",
        "\n",
        "# Top/bottom\n",
        "top_strike = strike_df_filtered.nlargest(10, 'Strike_Rate')\n",
        "bottom_strike = strike_df_filtered.nsmallest(10, 'Strike_Rate')\n",
        "\n",
        "# Preview\n",
        "print(\"‚úÖ Strike rates computed (min 50 bets).\")\n",
        "print(strike_df_filtered.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrBPZlTccebQ",
        "outputId": "99509cfa-9ccb-4c18-cfc2-d71e20f2b355"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Strike rates computed (min 50 bets).\n",
            "              Sport   Track_Name  total_bets  wins  Strike_Rate\n",
            "0  Greyhound Racing  Albion Park         305   178     0.583607\n",
            "1  Greyhound Racing   Angle Park         159    90     0.566038\n",
            "2  Greyhound Racing     Ballarat         218   118     0.541284\n",
            "3  Greyhound Racing      Bendigo         156    73     0.467949\n",
            "6  Greyhound Racing   Cannington         351   204     0.581197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 8: Prepare all_sheets for export ---\n",
        "\n",
        "all_sheets = {\n",
        "    'By Day':         by_day,\n",
        "    'By Day Sorted':  by_day,\n",
        "    'By Week':        by_week,\n",
        "    'Cumulative':     by_day[['Day','Cumulative_Profit_Loss']].rename(\n",
        "                         columns={'Cumulative_Profit_Loss':'Cumulative'}),\n",
        "    'By Month':       by_month,\n",
        "    'By Sport':       by_sport,\n",
        "    'By Country':     by_country,\n",
        "    'Track Stats':    track_stats,\n",
        "    'Top Strike Rates':    top_strike,\n",
        "    'Bottom Strike Rates': bottom_strike,\n",
        "    **tracks,\n",
        "    **sport_daily\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Prepared {len(all_sheets)} tables for Sheets export\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-sN4D9cgqN",
        "outputId": "ee3f6c4a-7a66-40e3-f7ac-6102fd7d33f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prepared 28 tables for Sheets export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä Top Horse Tracks preview:\")\n",
        "print(tracks['Top Horse Tracks'].head())\n",
        "print(\"üìä Bottom Horse Tracks preview:\")\n",
        "print(tracks['Bottom Horse Tracks'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfU0nlBeNY4n",
        "outputId": "f207a09f-9b61-4f34-8c6b-bca9373eac6f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Top Horse Tracks preview:\n",
            "            Sport Track_Name  Profit_Loss\n",
            "255  Horse Racing  Newcastle      1560.61\n",
            "315  Horse Racing  Southwell      1551.13\n",
            "169  Horse Racing  Geraldton      1400.78\n",
            "57   Horse Racing    Aintree      1270.77\n",
            "297  Horse Racing   Rosehill      1198.06\n",
            "üìä Bottom Horse Tracks preview:\n",
            "            Sport    Track_Name  Profit_Loss\n",
            "344  Horse Racing  Turfway Park      -336.93\n",
            "117  Horse Racing      Chepstow      -155.16\n",
            "215  Horse Racing     Lingfield      -149.08\n",
            "360  Horse Racing     Wincanton      -146.82\n",
            "292  Horse Racing         Ripon      -105.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 9: Export to Google Sheets ---\n",
        "\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "from datetime import date\n",
        "\n",
        "# 1Ô∏è‚É£ Authenticate\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# 2Ô∏è‚É£ Open sheet\n",
        "SHEET_NAME = 'Betfair Dashboard'\n",
        "sh = next((s for s in gc.openall() if s.title == SHEET_NAME), None)\n",
        "if not sh:\n",
        "    raise Exception(f\"Sheet '{SHEET_NAME}' not found.\")\n",
        "print(f\"‚úÖ Connected to '{SHEET_NAME}'\")\n",
        "\n",
        "# 3Ô∏è‚É£ Export each DataFrame\n",
        "for name, df_out in all_sheets.items():\n",
        "    # Format Profit_Loss\n",
        "    if 'Profit_Loss' in df_out:\n",
        "        df_out['Profit_Loss'] = pd.to_numeric(df_out['Profit_Loss'], errors='coerce').round(2)\n",
        "        df_out['Profit_Loss'] = df_out['Profit_Loss'].apply(\n",
        "            lambda x: f\"{x:.2f}\" if pd.notnull(x) else \"\"\n",
        "        )\n",
        "    # Convert week to string\n",
        "    if 'Week Starting' in df_out:\n",
        "        df_out['Week Starting'] = df_out['Week Starting'].astype(str)\n",
        "    # Round other numerics\n",
        "    for c in df_out.select_dtypes(['float','int']):\n",
        "        df_out[c] = df_out[c].round(2)\n",
        "\n",
        "    # Clear or create tab\n",
        "    try:\n",
        "        ws = sh.worksheet(name)\n",
        "        ws.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=name, rows=1000, cols=20)\n",
        "\n",
        "    set_with_dataframe(ws, df_out)\n",
        "    print(f\"‚úÖ Uploaded '{name}'\")\n",
        "\n",
        "# 4Ô∏è‚É£ Optional KPI Dashboard\n",
        "try:\n",
        "    dash = sh.worksheet('Dashboard')\n",
        "    dash.clear()\n",
        "except gspread.exceptions.WorksheetNotFound:\n",
        "    dash = sh.add_worksheet('Dashboard', rows=10, cols=5)\n",
        "\n",
        "total_profit = round(df['Profit_Loss'].sum(), 2)\n",
        "total_bets   = len(df)\n",
        "best_day     = df.groupby(df['Settled date'].dt.date)['Profit_Loss'].sum().idxmax()\n",
        "worst_day    = df.groupby(df['Settled date'].dt.date)['Profit_Loss'].sum().idxmin()\n",
        "kpis = [\n",
        "    ['Metric','Value'],\n",
        "    ['Total Profit/Loss', total_profit],\n",
        "    ['Number of Bets', total_bets],\n",
        "    ['Best Day', str(best_day)],\n",
        "    ['Worst Day', str(worst_day)],\n",
        "    ['Generated on', str(date.today())]\n",
        "]\n",
        "dash.update('A1', kpis)\n",
        "print(\"‚úÖ Dashboard KPIs updated\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw42u_CuCOsx",
        "outputId": "bdf5bbe5-cd72-4a35-cb28-a7692b3144dc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Connected to 'Betfair Dashboard'\n",
            "‚úÖ Uploaded 'By Day'\n",
            "‚úÖ Uploaded 'By Day Sorted'\n",
            "‚úÖ Uploaded 'By Week'\n",
            "‚úÖ Uploaded 'Cumulative'\n",
            "‚úÖ Uploaded 'By Month'\n",
            "‚úÖ Uploaded 'By Sport'\n",
            "‚úÖ Uploaded 'By Country'\n",
            "‚úÖ Uploaded 'Track Stats'\n",
            "‚úÖ Uploaded 'Top Strike Rates'\n",
            "‚úÖ Uploaded 'Bottom Strike Rates'\n",
            "‚úÖ Uploaded 'Top Horse Tracks'\n",
            "‚úÖ Uploaded 'Bottom Horse Tracks'\n",
            "‚úÖ Uploaded 'Top Greyhound Tracks'\n",
            "‚úÖ Uploaded 'Bottom Greyhound Tracks'\n",
            "‚úÖ Uploaded 'Snooker Daily'\n",
            "‚úÖ Uploaded 'Ice Hockey Daily'\n",
            "‚úÖ Uploaded 'Horse Racing Daily'\n",
            "‚úÖ Uploaded 'Golf Daily'\n",
            "‚úÖ Uploaded 'Politics Daily'\n",
            "‚úÖ Uploaded 'Tennis Daily'\n",
            "‚úÖ Uploaded 'Greyhound Racing Daily'\n",
            "‚úÖ Uploaded 'Football Daily'\n",
            "‚úÖ Uploaded 'Motor Sport Daily'\n",
            "‚úÖ Uploaded 'Cricket Daily'\n",
            "‚úÖ Uploaded 'Darts Daily'\n",
            "‚úÖ Uploaded 'Basketball Daily'\n",
            "‚úÖ Uploaded 'American Football Daily'\n",
            "‚úÖ Uploaded 'Rugby Union Daily'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-35-2198114328.py:66: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  dash.update('A1', kpis)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dashboard KPIs updated\n"
          ]
        }
      ]
    }
  ]
}